{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM23euRSWXDO2w90/UhCp2q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sri-7198/ENCORED/blob/main/airline_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efq5zYrZkn5h",
        "outputId": "fbb4cb2e-2fbc-49fa-a309-925e5c6c7509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of the text data:\n",
            "0                  @VirginAmerica What @dhepburn said.\n",
            "1    @VirginAmerica plus you've added commercials t...\n",
            "2    @VirginAmerica I didn't today... Must mean I n...\n",
            "3    @VirginAmerica it's really aggressive to blast...\n",
            "4    @VirginAmerica and it's a really big bad thing...\n",
            "Name: text, dtype: object\n",
            "Preprocessed text data:\n",
            "0                      [virginamerica, dhepburn, said]\n",
            "1    [virginamerica, plus, youve, added, commercial...\n",
            "2    [virginamerica, didnt, today, must, mean, need...\n",
            "3    [virginamerica, really, aggressive, blast, obn...\n",
            "4             [virginamerica, really, big, bad, thing]\n",
            "Name: clean_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Tweets.csv\"  # Update with the actual file path\n",
        "twitter_data = pd.read_csv(file_path)\n",
        "\n",
        "# Explore the Text Data\n",
        "print(\"Sample of the text data:\")\n",
        "print(twitter_data['text'].head())\n",
        "\n",
        "# Text Preprocessing\n",
        "# Remove punctuation\n",
        "twitter_data['clean_text'] = twitter_data['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "# Tokenization\n",
        "twitter_data['clean_text'] = twitter_data['clean_text'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Remove Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "twitter_data['clean_text'] = twitter_data['clean_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Verify Preprocessing\n",
        "print(\"Preprocessed text data:\")\n",
        "print(twitter_data['clean_text'].head())\n"
      ]
    }
  ]
}